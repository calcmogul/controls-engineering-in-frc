\section{Linear-quadratic regulator} \label{sec:lqr}
\index{controller design!linear-quadratic regulator}
\index{optimal control!linear-quadratic regulator}

\subsection{The intuition}

We can demonstrate the basic idea behind the linear-quadratic regulator with the
following flywheel model.
\begin{equation*}
  \dot{x} = ax + bu
\end{equation*}

where $a$ is a negative constant representing the back-EMF of the motor, $x$ is
the angular velocity, $b$ is a positive constant that maps the input voltage to
some change in angular velocity (angular acceleration), $u$ is the voltage
applied to the motor, and $\dot{x}$ is the angular acceleration. Discretized,
this equation would look like
\begin{equation*}
  x_{k+1} = a_d x + b_d u_k
\end{equation*}

If the angular velocity starts from zero and we apply a positive voltage, we'd
see the motor spin up to some constant speed following an exponential decay,
then stay at that speed. If we throw in the control law $u_k = k_p(r_k - x_k)$,
we can make the system converge to a desired state $r_k$ through proportional
feedback. In what manner can we pick the constant $k_p$ that balances getting to
the target angular velocity quickly with getting there efficiently (minimal
oscillations or excessive voltage)?

We can solve this problem with something called the linear-quadratic regulator.
We'll define the following cost function that includes the states and inputs:
\begin{equation*}
  J = \sum_{k=0}^\infty (Q(r_k - x_k)^2 + Ru_k^2)
\end{equation*}

We want to minimize this while obeying the constraint that the system follow our
flywheel dynamics $x_{k+1} = a_d x_k + b_d u_k$.

The cost is the sum of the squares of the error and the input for all time. If
the controller gain $k_p$ we pick in the control law $u_k = k_p(r_k - x_k)$ is
stable, the error $r_k - x_k$ and the input $u_k$ will both go to zero and give
us a finite cost. $Q$ and $R$ let us decide how much the error and input
contribute to the cost (we will require that $Q \geq 0$ and $R > 0$ for reasons
that will be clear shortly\footnote{Lets consider the boundary conditions on the
weights $Q$ and $R$. If we set $Q$ to zero, error doesn't contribute to the
cost, so the optimal solution is to not move. This minimizes the sum of the
inputs over time. If we let $R$ be zero, the input doesn't contribute to the
cost, so infinite inputs are allowed as they minimize the sum of the errors over
time. This isn't useful, so we require that the input be penalized with a
nonzero $R$.}). Penalizing error more will make the controller more aggressive,
while penalizing the input more will make the controller less aggressive. We
want to pick a $k_p$ that minimizes the cost.

There's a common trick for finding the value of a variable that minimizes a
function of that variable. We'll take the derivative (the slope) of the cost
function with respect to the input $u_k$, set the derivative to zero, then solve
for $u_k$. When the slope is zero, the function is at a minimum or maximum. Now,
the cost function we picked is quadratic. All the terms are strictly positive on
account of the squared variables and nonnegative weights, so our cost is
strictly positive and the quadratic function is concave up. The $u_k$ we found
is therefore a minimum.

The actual process of solving for $u_k$ is mathematically intensive and outside
the scope of this explanation (appendix \ref{ch:deriv_lqr} references a
derivation for those curious). The rest of this section will describe the more
general form of the linear-quadratic regulator and how to use it.

\subsection{The mathematical definition}

Instead of placing the poles of a closed-loop \gls{system} manually, the
linear-quadratic regulator (LQR) places the poles for us based on acceptable
relative \gls{error} and \gls{control effort} costs. This method of controller
design uses a quadratic function for the cost-to-go defined as the sum of the
\gls{error} and \gls{control effort} over time for the linear \gls{system}
$\mat{x}_{k+1} = \mat{A}\mat{x}_k + \mat{B}\mat{u}_k$.
\begin{equation*}
  J = \sum_{k=0}^\infty \left(\mat{x}_k\T\mat{Q}\mat{x}_k +
    \mat{u}_k\T\mat{R}\mat{u}_k\right)
\end{equation*}

where $J$ represents a trade-off between \gls{state} excursion and
\gls{control effort} with the weighting factors $\mat{Q}$ and $\mat{R}$. LQR is
a \gls{control law} $\mat{u}$ that minimizes the cost functional. Figure
\ref{fig:cost-to-go} shows the optimal cost-to-go for an elevator model. Pole
placement, on the other hand, will have a cost-to-go above this for an arbitrary
state vector (in this case, an arbitrary position-velocity pair).
\begin{svg}{build/\chapterpath/cost_to_go}
  \caption{Cost-to-go for elevator model}
  \label{fig:cost-to-go}
\end{svg}

The cost-to-go looks effectively constant along the velocity axis because the
velocity is contributing much less to the cost-to-go than
position.\footnote{While it may not look like it at this scale, the elevator's
cost-to-go along both state axes is strictly increasing away from the origin
(convex upward). This means there's a global minimum at the origin, and the
system is globally stable; no matter what state you start from, you're
guaranteed to reach the origin with this controller.} In other words, it's much
more expensive to correct for a position error than a velocity error. This
difference in cost is reflected by LQR's selected position feedback gain of
\input{build/\chapterpath/cost-to-go-kp}\unskip~and selected velocity feedback
gain of \input{build/\chapterpath/cost-to-go-kd}\unskip.

The minimum of LQR's cost functional is found by setting the derivative of the
cost functional to zero and solving for the \gls{control law} $\mat{u}_k$.
However, matrix calculus is used instead of normal calculus to take the
derivative.

The feedback \gls{control law} that minimizes $J$ is shown in theorem
\ref{thm:linear-quadratic_regulator}.
\begin{theorem}[Linear-quadratic regulator]
  \label{thm:linear-quadratic_regulator}
  \begin{align}
    \mat{u}_k^* = \argmin_{\mat{u}_k} &\sum\limits_{k=0}^\infty
      \left(\mat{x}_k\T\mat{Q}\mat{x}_k + \mat{u}_k\T\mat{R}\mat{u}_k\right)
      \nonumber \\
    \text{subject to } &\mat{x}_{k+1} = \mat{A}\mat{x}_k + \mat{B}\mat{u}_k
  \end{align}

  If the \gls{system} is controllable, the optimal control policy $\mat{u}_k^*$
  that drives all the \glspl{state} to zero is $-\mat{K}\mat{x}_k$. To converge
  to nonzero \glspl{state}, a \gls{reference} vector $\mat{r}_k$ can be added to
  the \gls{state} $\mat{x}_k$.
  \begin{equation}
    \mat{u}_k = \mat{K}(\mat{r}_k - \mat{x}_k)
  \end{equation}
\end{theorem}
\index{controller design!linear-quadratic regulator!definition}
\index{optimal control!linear-quadratic regulator!definition}

This means that optimal control can be achieved with simply a set of
proportional gains on all the \glspl{state}. To use the \gls{control law}, we
need knowledge of the full \gls{state} of the \gls{system}. That means we either
have to measure all our \glspl{state} directly or estimate those we do not
measure.

See appendix \ref{ch:deriv_lqr} for how $\mat{K}$ is calculated. If the result
is finite, the controller is guaranteed to be stable and
\glslink{robustness}{robust} with a \gls{gain margin} of infinity and a
\gls{phase margin} of 60 degrees \cite{bib:lqr_phase_margin}. However, using a
state estimator forfeits the robustness guarantees
\cite{bib:lqg_guaranteed_margins}.
\begin{remark}
  LQR design's $\mat{Q}$ and $\mat{R}$ matrices don't need \gls{discretization},
  but the $\mat{K}$ calculated for continuous time and discrete time
  \glspl{system} will be different. The discrete time gains approach the
  continuous time gains as the sample period tends to zero.
\end{remark}

\subsection{Bryson's rule}
\index{controller design!linear-quadratic regulator!Bryson's rule}
\index{optimal control!linear-quadratic regulator!Bryson's rule}

The next obvious question is what values to choose for $\mat{Q}$ and $\mat{R}$.
While this can be more of an art than a science, Bryson's rule provides a good
starting point. With Bryson's rule, the diagonals of the $\mat{Q}$ and $\mat{R}$
matrices are chosen based on the maximum acceptable value for each \gls{state}
and actuator. The nondiagonal elements are zero. The balance between state
excursion and control effort penalty ($\mat{Q}$ and $\mat{R}$ respectively) can
be adjusted using the weighting factor $\rho$.
\begin{equation*}
  \mat{Q} = \diag\left(\frac{\rho}{\mat{x}_{max}^2}\right)
  \quad
  \mat{R} = \diag\left(\frac{1}{\mat{u}_{max}^2}\right)
\end{equation*}

where $\mat{x}_{max}$ is the vector of acceptable state tolerances,
$\mat{u}_{max}$ is the vector of acceptable input tolerances, and $\rho$ is a
tuning constant. Small values of $\rho$ penalize \gls{control effort} while
large values of $\rho$ penalize \gls{state} excursions. Large values would be
chosen in applications like fighter jets where performance is necessary.
Spacecrafts would use small values to conserve their limited fuel supply.

\subsection{Pole placement vs LQR}

This example uses the following continuous second-order \gls{model} for a CIM
motor (a DC motor).
\begin{align*}
  \mat{A} = \begin{bmatrix}
    -\frac{b}{J} & \frac{K_t}{J} \\
    -\frac{K_e}{L} & -\frac{R}{L}
  \end{bmatrix}
  \quad
  \mat{B} = \begin{bmatrix}
    0 \\
    \frac{1}{L}
  \end{bmatrix}
  \quad
  \mat{C} = \begin{bmatrix}
    1 & 0
  \end{bmatrix}
  \quad
  \mat{D} = \begin{bmatrix}
    0
  \end{bmatrix}
\end{align*}

Figure \ref{fig:case_study_pp_lqr} shows the response using various discrete
pole placements and using LQR with the following cost matrices.
\begin{align*}
  \mat{Q} = \begin{bmatrix}
    \frac{1}{20^2} & 0 \\
    0 & 0
  \end{bmatrix}
  \quad
  \mat{R} = \begin{bmatrix}
    \frac{1}{12^2}
  \end{bmatrix}
\end{align*}

With Bryson's rule, this means an angular velocity tolerance of $20$ rad/s, an
infinite current tolerance (in other words, we don't care what the current
does), and a voltage tolerance of $12$ V.
\begin{svg}{build/\chapterpath/case_study_pp_lqr}
  \caption{Second-order CIM motor response with pole placement and LQR}
  \label{fig:case_study_pp_lqr}
\end{svg}

Notice with pole placement that as the current pole moves toward the origin, the
\gls{control effort} becomes more aggressive.
