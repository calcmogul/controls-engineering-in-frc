\chapter{Derivations}

\section{Transfer function in feedback}
\label{sec:deriv-tf-feedback}

Given the feedback network in figure \ref{fig:closed_loop_deriv}, find an
expression for $Y(s)$.

\begin{bookfigure}
  \begin{tikzpicture}[auto, >=latex']
    % Place the blocks
    \node [name=input] {$X(s)$};
    \node [sum, right=of input] (sum) {};
    \node [block, right=of sum] (G) {$G(s)$};
    \node [right=of G] (output) {$Y(s)$};
    \node [block, below=of G] (measurements) {$H(s)$};

    % Connect the nodes
    \draw [arrow] (input) -- node[pos=0.85] {$+$} (sum);
    \draw [arrow] (sum) -- node {$Z(s)$} (G);
    \draw [arrow] (G) -- node [name=y] {} (output);
    \draw [arrow] (y) |- (measurements);
    \draw [arrow] (measurements) -| node[pos=0.99, right] {$-$} (sum);
  \end{tikzpicture}

  \caption{Closed loop block diagram}
  \label{fig:closed_loop_deriv}
\end{bookfigure}

\begin{align}
  Y(s) &= Z(s) G(s) \nonumber \\
  Z(s) &= X(s) - Y(s) H(s) \nonumber \\
  X(s) &= Z(s) + Y(s) H(s) \nonumber \\
  X(s) &= Z(s) + Z(s) G(s) H(s) \nonumber \\
  \frac{Y(s)}{X(s)} &= \frac{Z(s) G(s)}{Z(s) + Z(s) G(s) H(s)} \nonumber \\
  \frac{Y(s)}{X(s)} &= \frac{G(s)}{1 + G(s) H(s)}
\end{align}

A more general form is

\begin{equation}
  \frac{Y(s)}{X(s)} = \frac{G(s)}{1 \mp G(s) H(s)}
\end{equation}

where positive feedback uses the top sign and negative feedback uses the bottom
sign.

\section{Optimal control law}
\label{sec:deriv-optimal-control-law}

For a continuous-time linear system described by

\begin{equation}
  \dot{\mtx{x}} = \mtx{A}\mtx{x} + \mtx{B}\mtx{u}
\end{equation}

with the cost function

\begin{equation*}
  J = \int\limits_0^\infty \left(\mtx{x}^T\mtx{Q}\mtx{x} +
    \mtx{u}^T\mtx{R}\mtx{u}\right) dt
\end{equation*}

where $J$ represents a tradeoff between \gls{state} excursion and control effort
with the weighting factors $\mtx{Q}$ and $\mtx{R}$, the feedback
\gls{control law} which minimizes $J$ is

\begin{equation*}
  \mtx{u} = -\mtx{K}\mtx{x}
\end{equation*}

where $\mtx{K}$ is given by

\begin{equation*}
  \mtx{K} = \mtx{R}^{-1} \left(\mtx{B}^T\mtx{P} + \mtx{N}^T\right)
\end{equation*}

and $\mtx{P}$ is found by solving the continuous-time algebraic Riccati equation
defined as

\begin{equation*}
  \mtx{A}^T\mtx{P} + \mtx{P}\mtx{A} - \left(\mtx{P}\mtx{B} +
    \mtx{N}\right) \mtx{R}^{-1} \left(\mtx{B}^T\mtx{P} + \mtx{N}^T\right) +
    \mtx{Q} = 0
\end{equation*}

or alternatively

\begin{equation*}
  \mathscrbf{A}^T\mtx{P} + \mtx{P}\mathscrbf{A} -
    \mtx{P}\mtx{B}\mtx{R}^{-1}\mtx{B}^T\mtx{P} + \mathscrbf{Q} = 0
\end{equation*}

with

\begin{align*}
  \mathscrbf{A} &= \mtx{A} - \mtx{B}\mtx{R}^{-1}\mtx{N}^T \\
  \mathscrbf{Q} &= \mtx{Q} - \mtx{N}\mtx{R}^{-1}\mtx{N}^T
\end{align*}

Snippet \ref{lst:dlqr} computes the optimal infinite-horizon, discrete-time
LQR controller.

\begin{code}{Python}{build/frccontrol/frccontrol/dlqr.py}
  \caption{Infinite-horizon, discrete-time LQR computation in Python}
  \label{lst:dlqr}
\end{code}

Other formulations of LQR for finite-horizon and discrete-time can be seen at
\url{https://en.wikipedia.org/wiki/Linearâ€“quadratic_regulator}.

MIT OpenCourseWare has a rigorous proof of the results shown above
\cite{bib:lqr-derivs}.

\section{Zero-order hold for state-space}
\label{sec:deriv-zoh-ss}

Starting with the continuous model

\begin{equation*}
  \dot{\mtx{x}}(t) = \mtx{A}\mtx{x}(t) + \mtx{B}\mtx{u}(t)
\end{equation*}

we know that the matrix exponential is

\begin{equation*}
  \frac{d}{dt}e^{\mtx{A}t} = \mtx{A}e^{\mtx{A}t} = e^{\mtx{A}t}\mtx{A}
\end{equation*}

and by premultiplying the model we get

\begin{equation*}
  e^{-\mtx{A}t}\dot{\mtx{x}}(t) = e^{-\mtx{A}t}\mtx{A}\mtx{x}(t) +
    e^{-\mtx{A}t}\mtx{B}\mtx{u}(t)
\end{equation*}

which we recognize as

\begin{equation*}
  \frac{d}{dt}\left(e^{-\mtx{A}t}\mtx{x}(t)\right) =
    e^{-\mtx{A}t}\mtx{B}\mtx{u}(t)
\end{equation*}

By integrating this equation, we get

\begin{align*}
  e^{-\mtx{A}t}\mtx{x}(t) - e^0\mtx{x}(0) &=
    \int_0^t e^{-\mtx{A}\tau}\mtx{B}\mtx{u}(\tau) \,d\tau \\
  \mtx{x}(t) &= e^{\mtx{A}t}\mtx{x}(0) +
    \int_0^t e^{\mtx{A}(t - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau
\end{align*}

which is an analytical solution to the continuous model. Now we want to
discretize it.

\begin{align*}
  \mtx{x}_k &\stackrel{def}{=} \mtx{x}(kT) \\
  \mtx{x}_k &= e^{\mtx{A}kT}\mtx{x}(0) +
    \int_0^{kT} e^{\mtx{A}(kT - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau \\
  \mtx{x}_{k+1} &= e^{\mtx{A}(k + 1)T}\mtx{x}(0) +
    \int_0^{(k + 1)T} e^{\mtx{A}((k + 1)T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau
    \\
  \mtx{x}_{k+1} &= e^{\mtx{A}(k + 1)T}\mtx{x}(0) +
    \int_0^{kT} e^{\mtx{A}((k + 1)T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau +
    \int_{kT}^{(k + 1)T} e^{\mtx{A}((k + 1)T - \tau)}\mtx{B}\mtx{u}(\tau)
    \,d\tau \\
  \mtx{x}_{k+1} &= e^{\mtx{A}(k + 1)T}\mtx{x}(0) +
    \int_0^{kT} e^{\mtx{A}((k + 1)T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau +
    \int_{kT}^{(k + 1)T} e^{\mtx{A}(kT + T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau
    \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T} \underbrace{\left(e^{\mtx{A}kT}\mtx{x}(0) +
    \int_0^{kT} e^{\mtx{A}(kT - \tau)}\mtx{B}\mtx{u}(\tau)
    \,d\tau\right)}_{\mtx{x}_k} +
    \int_{kT}^{(k + 1)T} e^{\mtx{A}(kT + T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau
\end{align*}

We assume that $\mtx{u}$ is constant during each timestep, so it can be pulled
out of the integral.

\begin{equation*}
  \mtx{x}_{k+1} = e^{\mtx{A}T}\mtx{x}_k +
    \left(\int_{kT}^{(k + 1)T} e^{\mtx{A}(kT + T - \tau)} \,d\tau\right)
    \mtx{B}\mtx{u}_k
\end{equation*}

The second term can be simplified by substituting it with the function
$v(\tau) = kT + T - \tau$. Note that $d\tau = -dv$.

\begin{align*}
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k -
    \left(\int_{v(kT)}^{v((k + 1)T)} e^{\mtx{A}v} \,dv\right)
    \mtx{B}\mtx{u}_k \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k -
    \left(\int_T^0 e^{\mtx{A}v} \,dv\right) \mtx{B}\mtx{u}_k \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k +
    \left(\int_0^T e^{\mtx{A}v} \,dv\right) \mtx{B}\mtx{u}_k \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k +
    \mtx{A}^{-1}(e^{\mtx{A}T} - \mtx{I}) \mtx{B}\mtx{u}_k
\end{align*}

which is an exact solution to the discretization problem.

\section{Kalman filter as Luenberger observer}
\label{sec:deriv-kalman-luenberger}

A Luenberger observer is defined as

\begin{align}
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k + \mtx{L}
    (\mtx{y}_k - \hat{\mtx{y}}_k) \label{eq:luenberger1} \\
  \hat{\mtx{y}}_k &= \mtx{C} \hat{\mtx{x}}_k^- \label{eq:luenberger2}
\end{align}

where a superscript of minus denotes \textit{a priori} and plus denotes
\textit{a posteriori} estimate. Combining equation (\ref{eq:luenberger1}) and
equation (\ref{eq:luenberger2}) gives

\begin{equation} \label{eq:luenberger}
  \hat{\mtx{x}}_{k+1}^+ = \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k + \mtx{L}
    (\mtx{y}_k - \mtx{C}\hat{\mtx{x}}_k^-)
\end{equation}

The following is a Kalman filter that considers the current update step and the
next predict step together rather than the current predict step and current
update step.

\begin{align}
  \text{Update step} \nonumber \\
  \mtx{K}_k &= \mtx{P}_k^- \mtx{H}^T (\mtx{H}\mtx{P}_k^- \mtx{H}^T +
    \mtx{R})^{-1} \\
  \hat{\mtx{x}}_k^+ &= \hat{\mtx{x}}_k^- + \mtx{K}_k(\mtx{y}_k -
    \mtx{H}\hat{\mtx{x}}_k^-) \label{eq:post2_x} \\
  \mtx{P}_k^+ &= (\mtx{I} - \mtx{K}_k\mtx{H})\mtx{P}_k^- \\
  \text{Predict step} \nonumber \\
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}\hat{\mtx{x}}_k^+ + \mtx{B}\mtx{u}_k
    \label{eq:pre2_x} \\
  \mtx{P}_{k+1}^- &= \mtx{A} \mtx{P}_k^+ \mtx{A}^T +
    \mtx{\Gamma}\mtx{Q}\mtx{\Gamma}^T
\end{align}

Substitute equation (\ref{eq:post2_x}) into equation (\ref{eq:pre2_x}).

\begin{align*}
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}(\hat{\mtx{x}}_k^- + \mtx{K}_k(\mtx{y}_k -
    \mtx{H}\hat{\mtx{x}}_k^-)) + \mtx{B}\mtx{u}_k \\
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}\mtx{x}_k^- + \mtx{A}\mtx{K}_k(\mtx{y}_k -
    \mtx{H}\hat{\mtx{x}}_k^-) + \mtx{B}\mtx{u}_k \\
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k +
    \mtx{A}\mtx{K}_k(\mtx{y}_k - \mtx{H}\hat{\mtx{x}}_k^-)
\end{align*}

Let $\mtx{C} = \mtx{H}$ and $\mtx{L} = \mtx{A} \mtx{K}_k$.

\begin{equation} \label{eq:app_kalman_leunberger}
  \hat{\mtx{x}}_{k+1}^+ = \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k + \mtx{L}
    (\mtx{y}_k - \mtx{C}\hat{\mtx{x}}_k^-)
\end{equation}

which matches equation (\ref{eq:luenberger}). Therefore, the eigenvalues of the
Kalman filter observer can be obtained by

\begin{align}
  &eig(\mtx{A} - \mtx{L}\mtx{C}) \nonumber \\
  &eig(\mtx{A} - (\mtx{A}\mtx{K}_k)(\mtx{H})) \nonumber \\
  &eig(\mtx{A}(\mtx{I} - \mtx{K}_k\mtx{H}))
\end{align}

\subsection{Luenberger observer with separate prediction and update}
\label{subsec:deriv-luenberger-separate}

To run a Luenberger observer with separate prediction and update steps,
substitute the relationship between the Luenberger observer and Kalman filter
matrices derived above into the Kalman filter equations.

Appendix \ref{sec:deriv-kalman-luenberger} shows that $\mtx{C} = \mtx{H}$ and
$\mtx{L} = \mtx{A}\mtx{K}_k$. Since $\mtx{L}$ and $\mtx{A}$ are constant, one
must assume $\mtx{K}_k$ has reached steady-state. Then,
$\mtx{K} = \mtx{A}^{-1}\mtx{L}$. Substitute this and $\mtx{C} = \mtx{H}$ into
the Kalman filter update equation.

\begin{align*}
  \hat{\mtx{x}}_{k+1}^+ &= \hat{\mtx{x}}_{k+1}^- + \mtx{K}(\mtx{y}_{k+1} -
    \mtx{H}\hat{\mtx{x}}_{k+1}^-) \\
  \hat{\mtx{x}}_{k+1}^+ &= \hat{\mtx{x}}_{k+1}^- + \mtx{A}^{-1}\mtx{L}
    (\mtx{y}_{k+1} - \mtx{C}\hat{\mtx{x}}_{k+1}^-)
\end{align*}

Substitute in equation (\ref{eq:z_obsv_y}).

\begin{equation*}
  \hat{\mtx{x}}_{k+1}^+ = \hat{\mtx{x}}_{k+1}^- + \mtx{A}^{-1}\mtx{L}
    (\mtx{y}_{k+1} - \hat{\mtx{y}}_{k+1})
\end{equation*}

The predict step is the same as the Kalman filter's. Therefore, a Luenberger
observer run with prediction and update steps is written as follows.

\begin{align}
  \text{Predict step} \nonumber \\
  \hat{\mtx{x}}_{k+1}^- &= \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k \\
  \text{Update step} \nonumber \\
  \hat{\mtx{x}}_{k+1}^+ &= \hat{\mtx{x}}_{k+1}^- + \mtx{A}^{-1}\mtx{L}
    (\mtx{y}_{k+1} - \hat{\mtx{y}}_{k+1}) \\
  \hat{\mtx{y}}_{k+1} &= \mtx{C} \hat{\mtx{x}}_{k+1}^-
\end{align}
