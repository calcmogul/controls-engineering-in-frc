\section{Introduction to probability}
\index{probability}

Now we'll begin establishing probability concepts we need to describe and
manipulate stochastic systems.

\subsection{Random variables}
\index{probability!random variables}

A random variable is a variable whose values are the outcomes of a random
phenomenon, like dice rolls or noisy process measurements. As such, a random
variable is defined as a function that maps the outcomes of an unpredictable
process to numerical quantities. A particular output of this function is called
a sample. The sample space is the set of possible values taken by the random
variable.

\index{probability!probability density function}
A probability density function (PDF) is a function of the random variable whose
value at a given sample (measured value) in the sample space (the range of
possible measured values) is the probability of that sample occurrring. The area
under the function over a range gives the probability that the sample falls
within that range. Let $x$ be a random variable, and let $p(x)$ denote the
probability density function of $x$. The probability that the value of $x$ will
be in the interval $x \in [x_1, x_1 + dx]$ is $p(x_1) \,dx$. In other words, the
probability is the area under the PDF within the region $[x_1, x_1 + dx]$ (see
figure \ref{fig:pdf}).
\begin{svg}{build/\chapterpath/pdf}
  \caption{Probability density function}
  \label{fig:pdf}
\end{svg}

A probability of zero means that the sample will not occur and a probability of
one means that the sample will always occur. Probability density functions
require that no probabilities are negative and that the sum of all probabilities
is $1$. If the probabilities sum to $1$, that means one of those outcomes
\textit{must} happen. In other words,
\begin{equation*}
  p(x) \geq 0, \int_{-\infty}^\infty p(x) \,dx = 1
\end{equation*}

or given that the probability of a given sample is greater than or equal to
zero, the sum of probabilities for all possible input values is equal to one.

\subsection{Expected value}
\index{probability!expected value}

Expected value or expectation is a weighted average of the values the PDF can
produce where the weight for each value is the probability of that value
occurring. This can be written mathematically as
\begin{equation*}
  \overline{x} = E[x] = \int_{-\infty}^\infty x \,p(x) \,dx
\end{equation*}

The expectation can be applied to random functions as well as random variables.
\begin{equation*}
  E[f(x)] = \int_{-\infty}^\infty f(x) \,p(x) \,dx
\end{equation*}

The mean of a random variable is denoted by an overbar (e.g., $\overline{x}$)
pronounced x-bar. The expectation of the difference between a random variable
and its mean $x - \overline{x}$ converges to zero. In other words, the
expectation of a random variable is its mean. Here's a proof.
\begin{align*}
  E[x - \overline{x}] &= \int_{-\infty}^\infty (x - \overline{x}) \,p(x) \,dx \\
  E[x - \overline{x}] &= \int_{-\infty}^\infty x \, p(x) \,dx -
    \int_{-\infty}^\infty \overline{x} \,p(x) \,dx \\
  E[x - \overline{x}] &= \int_{-\infty}^\infty x \,p(x) \,dx -
    \overline{x} \int_{-\infty}^\infty p(x) \,dx \\
  E[x - \overline{x}] &= \overline{x} - \overline{x} \cdot 1 \\
  E[x - \overline{x}] &= 0 \\
\end{align*}

\subsection{Variance}
\index{probability!variance}

Informally, variance is a measure of how far the outcome of a random variable
deviates from its mean. Later, we will use variance to quantify how confident we
are in the estimate of a random variable; we can't know the true value of that
variable without randomness, but we can give a bound on its randomness.
\begin{equation*}
  \var(x) = \sigma^2 = E[(x - \overline{x})^2] =
    \int_{-\infty}^{\infty} (x - \overline{x})^2 \,p(x) \,dx
\end{equation*}

The standard deviation is the square root of the variance.
\begin{equation*}
  \std[x] = \sigma = \sqrt{\var(x)}
\end{equation*}

\subsection{Joint probability density functions}
\index{probability!probability density functions}

Probability density functions can also include more than one variable. Let $x$
and $y$ be random variables. The joint probability density function $p(x, y)$
defines the probability $p(x, y) \,dx \,dy$, so that $x$ and $y$ are in the
intervals $x \in [x, x + dx], y \in [y, y + dy]$. In other words, the
probability is the volume under a region of the PDF manifold (see figure
\ref{fig:joint_pdf} for an example of a joint PDF).
\begin{svg}{build/\chapterpath/joint_pdf}
  \caption{Joint probability density function}
  \label{fig:joint_pdf}
\end{svg}

Joint probability density functions also require that no probabilities are
negative and that the sum of all probabilities is $1$.
\begin{equation*}
  p(x, y) \geq 0, \int_{-\infty}^\infty \int_{-\infty}^{\infty} p(x, y) \,dx
    \,dy = 1
\end{equation*}

The expected values for joint PDFs are as follows.
\begin{align*}
  E[x] &= \int_{-\infty}^\infty \int_{-\infty}^{\infty} x \,dx \,dy \\
  E[y] &= \int_{-\infty}^\infty \int_{-\infty}^{\infty} y \,dx \,dy \\
  E[f(x, y)] &= \int_{-\infty}^\infty \int_{-\infty}^{\infty} f(x, y) \,dx \,dy
\end{align*}

The variance of a joint PDF measures how a variable correlates with itself
(we'll cover variances with respect to other variables shortly).
\begin{align*}
  \var(x) &= \Sigma_{xx} = E[(x - \overline{x})^2] =
    \int_{-\infty}^\infty \int_{-\infty}^\infty (x - \overline{x})^2 \,p(x, y)
    \,dx \,dy \\
  \var(y) &= \Sigma_{yy} = E[(y - \overline{y})^2] =
    \int_{-\infty}^\infty \int_{-\infty}^\infty (y - \overline{y})^2 \,p(x, y)
    \,dx \,dy \\
\end{align*}

\subsection{Covariance}
\index{probability!covariance}

A covariance is a measurement of how a variable correlates with another. If they
vary in the same direction, the covariance increases. If they vary in opposite
directions, the covariance decreases.
\begin{equation*}
  \cov(x, y) = \Sigma_{xy} = E[(x - \overline{x})(y - \overline{y})] =
    \int_{-\infty}^\infty \int_{-\infty}^\infty (x - \overline{x})
    (y - \overline{y}) \,p(x, y) \,dx \,dy \\
\end{equation*}

\subsection{Correlation}

Two random variables are correlated if the result of one random variable affects
the result of another. Correlation is defined as
\begin{equation*}
  \rho(x, y) = \frac{\Sigma_{xy}}{\sqrt{\Sigma_{xx}\Sigma_{yy}}}, |\rho(x, y)|
    \leq 1
\end{equation*}

So two variable's correlation is defined as their covariance over the geometric
mean of their variances. Uncorrelated sources have a covariance of zero.

\subsection{Independence}

Two random variables are independent if the following relation is true.
\begin{equation*}
  p(x, y) = p(x) \,p(y)
\end{equation*}

This means that the values of $x$ do not correlate with the values of $y$. That
is, the outcome of one random variable does not affect another's outcome. If we
assume independence,
\begin{align*}
  E[xy] &= \int_{-\infty}^\infty \int_{-\infty}^\infty xy \,p(x, y) \,dx \,dy \\
  E[xy] &= \int_{-\infty}^\infty \int_{-\infty}^\infty xy \,p(x) \,p(y) \,dx
    \,dy \\
  E[xy] &= \int_{-\infty}^\infty x \,p(x) \,dx \int_{-\infty}^\infty y \,p(y)
    \,dy \\
  E[xy] &= E[x]E[y] \\
  E[xy] &= \overline{x}\,\overline{y}
\end{align*}
\begin{align*}
  \cov(x, y) &= E[(x - \overline{x})(y - \overline{y})] \\
  \cov(x, y) &= E[(x - \overline{x})]E[(y - \overline{y})] \\
  \cov(x, y) &= 0 \cdot 0 \\
\end{align*}

Therefore, the covariance $\Sigma_{xy}$ is zero, as expected. Furthermore,
$\rho(x, y) = 0$, which means they are uncorrelated.

\subsection{Marginal probability density functions}
\index{probability!marginal probability density functions}

Given two random variables $x$ and $y$ whose joint distribution is known, the
marginal PDF $p(x)$ expresses the probability of $x$ averaged over information
about $y$. In other words, it's the PDF of $x$ when $y$ is unknown. This is
calculated by integrating the joint PDF over $y$.
\begin{equation*}
  p(x) = \int_{-\infty}^\infty p(x, y) \,dy
\end{equation*}

\subsection{Conditional probability density functions}
\index{probability!conditional probability density functions}

Let us assume that we know the joint PDF $p(x, y)$ and the exact value for $y$.
The conditional PDF gives the probability of $x$ in the interval $[x, x + dx]$
for the given value $y$.

If $p(x, y)$ is known, then we also know $p(x, y = y^\ast)$. However, note that
the latter is not the conditional density $p(x|y^\ast)$, instead
\begin{align*}
  C(y^\ast) &= \int_{-\infty}^\infty p(x, y = y^\ast) \,dx \\
  p(x|y^\ast) &= \frac{1}{C(y^\ast)} p(x, y = y^\ast)
\end{align*}

The scale factor $\frac{1}{C(y^\ast)}$ is used to scale the area under the PDF
to $1$.

\subsection{Bayes's rule}
\index{probability!Bayes's rule}

Bayes's rule is used to determine the probability of an event based on prior
knowledge of conditions related to the event.
\begin{equation*}
  p(x, y) = p(x|y) \,p(y) = p(y|x) \,p(x)
\end{equation*}

If $x$ and $y$ are independent, then $p(x|y) = p(x)$, $p(y|x) = p(y)$, and
$p(x, y) = p(x) \,p(y)$.

\subsection{Conditional expectation}
\index{probability!conditional expectation}

The concept of expectation can also be applied to conditional PDFs. This allows
us to determine what the mean of a variable is given prior knowledge of other
variables.
\begin{align*}
  E[x|y] &= \int_{-\infty}^\infty x \,p(x|y) \,dx = f(y), E[x|y] \neq E[x] \\
  E[y|x] &= \int_{-\infty}^\infty y \,p(y|x) \,dy = f(x), E[y|x] \neq E[y]
\end{align*}

\subsection{Conditional variances}
\index{probability!conditional variances}
\begin{align*}
  \var(x|y) &= E[(x - E[x|y])^2|y] \\
  \var(x|y) &= \int_{-\infty}^\infty (x - E[x|y])^2 \,p(x|y) \,dx
\end{align*}

\subsection{Random vectors}

Now we will extend the probability concepts discussed so far to vectors where
each element has a PDF.
\begin{equation*}
  \mat{x} = \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
  \end{bmatrix}
\end{equation*}

The elements of $\mat{x}$ are scalar variables jointly distributed with a joint
density $p(x_1, \ldots, x_n)$. The expectation is
\begin{align*}
  E[\mat{x}] &= \matbar{x} = \int_{-\infty}^\infty \mat{x} \,p(\mat{x})
    \,d\mat{x} \\
  E[\mat{x}] &= \begin{bmatrix}
    E[x_1] \\
    \vdots \\
    E[x_n]
  \end{bmatrix} \\
  E[x_i] &= \int_{-\infty}^\infty \ldots \int_{-\infty}^\infty x_i
    \,p(x_1, \ldots, x_n) \,dx_1 \ldots dx_n \\
  E[f(\mat{x})] &= \int_{-\infty}^\infty f(\mat{x}) \,p(\mat{x}) \,d\mat{x}
\end{align*}

\subsection{Covariance matrix}
\index{probability!covariance matrix}

The covariance matrix for a random vector $\mat{x} \in \mathbb{R}^n$ is
\begin{align*}
  \mat{\Sigma} &= \cov(\mat{x}, \mat{x}) = E[(\mat{x} - \matbar{x})
    (\mat{x} - \matbar{x})\T] \\
  \mat{\Sigma} &= \begin{bmatrix}
    \cov(x_1, x_1) & \ldots & \cov(x_1, x_n) \\
    \vdots         & \ddots & \vdots \\
    \cov(x_n, x_1) & \ldots & \cov(x_n, x_n) \\
  \end{bmatrix}
\end{align*}

This $n \times n$ matrix is symmetric and positive semidefinite. A positive
semidefinite matrix satisfies the relation that for any
$\mat{v} \in \mathbb{R}^n$ for which $\mat{v} \neq 0$,
$\mat{v}\T \mat{\Sigma} \mat{v} \geq 0$. In other words, the eigenvalues of
$\mat{\Sigma}$ are all greater than or equal to zero.

\subsection{Relations for independent random vectors}

First, independent vectors imply linearity from
$p(\mat{x}, \mat{y}) = p(\mat{x}) \,p(\mat{y})$.
\begin{align*}
  E[\mat{A}\mat{x} + \mat{B}\mat{y}] &= \mat{A}E[\mat{x}] + \mat{B}E[\mat{y}] \\
  E[\mat{A}\mat{x} + \mat{B}\mat{y}] &= \mat{A}\matbar{x} + \mat{B}\matbar{y}
\end{align*}

Second, independent vectors being uncorrelated means their covariance is zero.
\begin{align}
  \mat{\Sigma}_{\mat{x}\mat{y}} &= \cov(\mat{x}, \mat{y}) \nonumber \\
  \mat{\Sigma}_{\mat{x}\mat{y}} &= E[(\mat{x} - \matbar{x})
    (\mat{y} - \matbar{y})\T] \nonumber \\
  \mat{\Sigma}_{\mat{x}\mat{y}} &= E[\mat{x}\mat{y}\T] -
    E[\mat{x}\matbar{y}\T] - E[\matbar{x}\mat{y}\T] +
    E[\matbar{x}\matbar{y}\T] \nonumber \\
  \mat{\Sigma}_{\mat{x}\mat{y}} &= E[\mat{x}\mat{y}\T] -
    E[\mat{x}]\matbar{y}\T - \matbar{x}E[\mat{y}\T] +
    \matbar{x}\matbar{y}\T \nonumber \\
  \mat{\Sigma}_{\mat{x}\mat{y}} &= E[\mat{x}\mat{y}\T] -
    \matbar{x}\matbar{y}\T - \matbar{x}\matbar{y}\T +
    \matbar{x}\matbar{y}\T \nonumber \\
  \mat{\Sigma}_{\mat{x}\mat{y}} &= E[\mat{x}\mat{y}\T] -
    \matbar{x}\matbar{y}\T \label{eq:prb_sigma}
\end{align}

Now, compute $E[\mat{x}\mat{y}\T]$.
\begin{equation*}
  E[\mat{x}\mat{y}\T] = \int_X \int_Y \mat{x}\mat{y}\T \,p(\mat{x})
    \,p(\mat{y}) \,d\mat{x} \,d\mat{y}\T
\end{equation*}

Factor out constants from the inner integral. This includes variables which are
held constant for each inner integral evaluation.
\begin{equation*}
  E[\mat{x}\mat{y}\T] = \int_X p(\mat{x}) \,\mat{x} \,d\mat{x}
    \int_Y p(\mat{y}) \,\mat{y}\T \,d\mat{y}\T
\end{equation*}

Each of these integrals is just the expected value of their respective
integration variable.
\begin{align}
  E[\mat{x}\mat{y}\T] &= \matbar{x}\matbar{y}\T \label{eq:prb_exyt}
\end{align}

Substitute equation \eqref{eq:prb_exyt} into equation \eqref{eq:prb_sigma}.
\begin{align*}
  \mat{\Sigma}_{\mat{x}\mat{y}} &= (\matbar{x}\matbar{y}\T) -
    \matbar{x}\matbar{y}\T \\
  \mat{\Sigma}_{\mat{x}\mat{y}} &= 0
\end{align*}

Using these results, we can compute the covariance of
$\mat{z} = \mat{A}\mat{x} + \mat{B}\mat{y}$.
\begin{align*}
  \Sigma_z &= \cov(\mat{z}, \mat{z}) \\
  \Sigma_z &= E[(\mat{z} - \matbar{z})(\mat{z} - \matbar{z})\T] \\
  \Sigma_z &= E[(\mat{A}\mat{x} + \mat{B}\mat{y} - \mat{A}\matbar{x} -
    \mat{B}\matbar{y})(\mat{A}\mat{x} + \mat{B}\mat{y} -
    \mat{A}\matbar{x} - \mat{B}\matbar{y})\T] \\
  \Sigma_z &= E[(\mat{A}(\mat{x} - \matbar{x}) +
    \mat{B}(\mat{y} - \matbar{y}))
    (\mat{A}(\mat{x} - \matbar{x}) +
     \mat{B}(\mat{y} - \matbar{y}))\T] \\
  \Sigma_z &= E[(\mat{A}(\mat{x} - \matbar{x}) +
    \mat{B}(\mat{y} - \matbar{y}))
    ((\mat{x} - \matbar{x})\T\mat{A}\T +
     (\mat{y} - \matbar{y})\T\mat{B}\T)] \\
  \Sigma_z &= E[
    \mat{A}(\mat{x} - \matbar{x})(\mat{x} - \matbar{x})\T\mat{A}\T +
    \mat{A}(\mat{x} - \matbar{x})(\mat{y} - \matbar{y})\T\mat{B}\T \\
    &\qquad + \mat{B}(\mat{y} - \matbar{y})(\mat{x} - \matbar{x})\T\mat{A}\T +
    \mat{B}(\mat{y} - \matbar{y})(\mat{y} - \matbar{y})\T\mat{B}\T]
  \intertext{Since $\mat{x}$ and $\mat{y}$ are independent, $\Sigma_{xy} = 0$
    and the cross terms cancel out.}
  \Sigma_z &= E[
    \mat{A}(\mat{x} - \matbar{x})(\mat{x} - \matbar{x})\T\mat{A}\T + 0 + 0 +
    \mat{B}(\mat{y} - \matbar{y})(\mat{y} - \matbar{y})\T\mat{B}\T] \\
  \Sigma_z &=
    E[\mat{A}(\mat{x} - \matbar{x})(\mat{x} - \matbar{x})\T\mat{A}\T] +
    E[\mat{B}(\mat{y} - \matbar{y})(\mat{y} - \matbar{y})\T\mat{B}\T] \\
  \Sigma_z &=
    \mat{A}E[(\mat{x} - \matbar{x})(\mat{x} - \matbar{x})\T]\mat{A}\T +
    \mat{B}E[(\mat{y} - \matbar{y})(\mat{y} - \matbar{y})\T]\mat{B}\T
  \intertext{Recall that $\Sigma_x = \cov(\mat{x}, \mat{x})$ and
    $\Sigma_y = \cov(\mat{y}, \mat{y})$.}
  \Sigma_z &= \mat{A}\Sigma_x\mat{A}\T + \mat{B}\Sigma_y\mat{B}\T
\end{align*}

\subsection{Gaussian random variables}

A Gaussian random variable has the following properties:
\begin{align*}
  E[x] &= \overline{x} \\
  \var(x) &= \sigma^2 \\
  p(x) &= \frac{1}{\sqrt{2\pi\sigma^2}}
    e^{-\frac{(x - \overline{x})^2}{2\sigma^2}}
\end{align*}

While we could use any random variable to represent a random process, we use the
Gaussian random variable a lot in probability theory due to the central limit
theorem.
\begin{definition}[Central limit theorem]
  When independent random variables are added, their properly normalized sum
  tends toward a normal distribution (a Gaussian distribution or ``bell curve").
\end{definition}
\index{probability!central limit theorem}

This is the case even if the original variables themselves are not normally
distributed. The theorem is a key concept in probability theory because it
implies that probabilistic and statistical methods that work for normal
distributions can be applicable to many problems involving other types of
distributions.

For example, suppose that a sample is obtained containing a large number of
independent observations, and that the arithmetic mean of the observed values
is computed. The central limit theorem says that the computed values of the
mean will tend toward being distributed according to a normal distribution.
