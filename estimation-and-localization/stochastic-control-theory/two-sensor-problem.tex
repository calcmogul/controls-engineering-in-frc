\section{Two-sensor problem}
\index{stochastic!two-sensor problem}

\subsection{Two noisy (independent) observations}

Variable $z_1$ is the noisy measurement of the variable $x$. The value of the
noisy measurement is the random variable defined by the probability density
function $p(z_1|x)$.

Variable $z_2$ is the noisy measurement of the same variable x and independent
of $z_1$. If we know the property of our sensor (measurement noise), then the
probability density function of the measurement is $p(z_2|x)$.

We are interested in the probability density function of $x$ given the
measurements $z_1$ and $z_2$.
\begin{equation*}
  p(x|z_1, z_2) = \frac{p(x, z_1, z_2)}{p(z_1, z_2)}
                = \frac{p(z_1, z_2|x)p(x)}{p(z_1, z_2)}
                = \frac{p(z_1|x) p(z_2|x) p(x)}{p(z_1, z_2)}
\end{equation*}

where $p(z_1, z_2) = \int_X p(z_1|x) p(z_2|x) \,dx$, but $z_1$ and $z_2$ are
given and we can write
\begin{equation*}
  p(x|z_1, z_2) = \frac{1}{C} p(z_1|x) p(z_2|x) p(x) \quad \text{or} \quad
    p(x|z_1, z_2) \sim p(z_1|x) p(z_2|x) p(x)
\end{equation*}

where $C$ is a normalizing constant providing that
$\int_X p(x|z_1, z_2) \,dx = 1$. The probability density $p(x|z_1, z_2)$
summarizes our complete knowledge about $x$.

\subsection{Single noisy observations}

Variable $z_1$ is the noisy measurement of the variable $x$. The value of the
noisy measurement is the random variable defined by the probability density
function $p(z_1|x)$.

The probability density function of $x$ given the measurement $z_1$ is
\begin{equation*}
  p(x|z_1) = \frac{p(x, z_1)}{p(z_1)} = \frac{p(z_1|x) p(x)}{p(z_1)}
\end{equation*}

where $p(z_1) = \int_X p(z_1|x) \,dx$, but $z_1$ is given so we can write
\begin{equation*}
  p(x|z_1) = \frac{1}{C} p(z_1|x) p(x) \quad \text{or} \quad
    p(x|z_1) \sim p(z_1|x) p(x)
\end{equation*}

The probability density $p(x|z_1)$ summarizes our complete knowledge about $x$.
\begin{remark}
  In both the single and double measurement cases, the estimation of the
  variable $x$ is a data/information fusion problem. In the single measurement
  case, we combine the \textit{prior probability} with the probability resulting
  from the measurement.
\end{remark}

\subsection{Single noisy observations: a Gaussian case}
\begin{equation*}
  p(z_1|x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}
    \left(\frac{z_1 - x}{\sigma}\right)^2}
  \quad \text{and} \quad
  p(x) = \frac{1}{\sigma_0 \sqrt{2\pi}} e^{-\frac{1}{2}
    \left(\frac{x - x_0}{\sigma_0}\right)^2}
\end{equation*}

$z_1$, $x_0$, $\sigma^2$, and $\sigma_0^2$ are given.
\begin{align*}
  p(x|z_1) &= \frac{p(x, z_1)}{p(z_1)} \\
  p(x|z_1) &= \frac{p(z_1|x) p(x)}{p(z_1)} \\
  p(x|z_1) &= \frac{1}{C} p(z_1|x) p(x) \\
  p(x|z_1) &= \frac{1}{C}
    \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}
      \left(\frac{z_1 - x}{\sigma}\right)^2}
    \frac{1}{\sigma_0 \sqrt{2\pi}} e^{-\frac{1}{2}
      \left(\frac{x - x_0}{\sigma_0}\right)^2}
  \intertext{Absorb the leading coefficients of the two probability
    distributions into a new constant $C_1$.}
  p(x|z_1) &= \frac{1}{C_1}
    e^{-\frac{1}{2} \left(\frac{z_1 - x}{\sigma}\right)^2}
    e^{-\frac{1}{2} \left(\frac{x - x_0}{\sigma_0}\right)^2}
  \intertext{Combine the exponents.}
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2} \left(
      \frac{(z_1 - x)^2}{\sigma^2} + \frac{(x - x_0)^2}{\sigma_0^2}
    \right)}
  \intertext{Expand the exponent into separate terms.}
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2} \left(
      \frac{z_1^2}{\sigma^2} - \frac{2z_1 x}{\sigma^2} + \frac{x^2}{\sigma^2} +
      \frac{x^2}{\sigma_0^2} - \frac{2xx_0}{\sigma_0^2} + \frac{x_0^2}{\sigma_0^2}
    \right)}
  \intertext{Multiply in
    $\frac{\sigma^2}{\sigma^2}$ or $\frac{\sigma_0^2}{\sigma_0^2}$ as
    appropriate to give all terms a denominator of $\sigma^2 \sigma_0^2$.}
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2} \left(
      \frac{\sigma_0^2 z_1^2}{\sigma^2 \sigma_0^2}
      - 2\frac{\sigma_0^2 z_1}{\sigma^2 \sigma_0^2} x
      + \frac{\sigma_0^2}{\sigma^2 \sigma_0^2} x^2
      + \frac{\sigma^2}{\sigma^2 \sigma_0^2} x^2
      - 2\frac{\sigma^2 x_0}{\sigma^2 \sigma_0^2} x
      + \frac{\sigma^2 x_0^2}{\sigma^2 \sigma_0^2}
    \right)}
  \intertext{Reorder terms in the exponent.}
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2} \left(
      \frac{\sigma_0^2}{\sigma^2 \sigma_0^2} x^2
      + \frac{\sigma^2}{\sigma^2 \sigma_0^2} x^2
      - 2\frac{\sigma_0^2 z_1}{\sigma^2 \sigma_0^2} x
      - 2\frac{\sigma^2 x_0}{\sigma^2 \sigma_0^2} x
      + \frac{\sigma_0^2 z_1^2}{\sigma^2 \sigma_0^2}
      + \frac{\sigma^2 x_0^2}{\sigma^2 \sigma_0^2}
    \right)}
  \intertext{Combine like terms.}
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2} \left(
      \frac{\sigma_0^2 + \sigma^2}{\sigma^2\sigma_0^2} x^2 -
      2\frac{\sigma_0^2 z_1 + \sigma^2 x_0}{\sigma^2 \sigma_0^2} x +
      \frac{\sigma_0^2 z_1^2 + \sigma^2 x_0^2}{\sigma^2 \sigma_0^2}
    \right)}
  \intertext{Factor out $\frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2}$.}
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2}
    \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2} \left(
      x^2 - 2\frac{\sigma_0^2 z_1 + \sigma^2 x_0}{\sigma_0^2 + \sigma^2} x +
      \frac{\sigma_0^2 z_1^2 + \sigma^2 x_0^2}{\sigma_0^2 + \sigma^2}
    \right)} \\
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2}
    \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2} \left(
      x^2 - 2\left(
        \frac{\sigma_0^2}{\sigma_0^2 + \sigma^2} z_1 +
        \frac{\sigma^2}{\sigma_0^2 + \sigma^2} x_0
      \right) x +
      \frac{\sigma_0^2 z_1^2 + \sigma^2 x_0^2}{\sigma_0^2 + \sigma^2}
    \right)}
  \intertext{$\frac{\sigma_0^2}{\sigma^2 + \sigma_0^2} z_1 + \frac{\sigma^2}{\sigma_0^2 + \sigma^2} x_0$
    is the mean of the combined probability distribution, which we'll denote as
    $\mu$.}
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2}
    \frac{\sigma^2 + \sigma_0^2}{\sigma^2 \sigma_0^2} \left(
      x^2 - 2\mu x +
      \frac{\sigma_0^2 z_1^2 + \sigma^2 x_0^2}{\sigma_0^2 + \sigma^2}
    \right)}
  \intertext{Add in $\mu^2 - \mu^2$ to perform some factoring.}
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2}
    \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2} \left(
      x^2 - 2\mu x + \mu^2 - \mu^2 +
      \frac{\sigma_0^2 z_1^2 + \sigma^2 x_0^2}{\sigma_0^2 + \sigma^2}
    \right)} \\
  p(x|z_1) &= \frac{1}{C_1} e^{-\frac{1}{2}
    \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2} \left(
      (x - \mu)^2 - \mu^2 +
      \frac{\sigma_0^2 z_1^2 + \sigma^2 x_0^2}{\sigma_0^2 + \sigma^2}
    \right)}
  \intertext{Pull out all constant terms in the exponent and combine them with
    $C_1$ to make a new constant $C_2$. We're basically doing
    $c_1 e^{x + a} \rightarrow c_1 e^a e^x \rightarrow c_2 e^x$.}
  p(x|z_1) &= \frac{1}{C_2} e^{-\frac{1}{2}
    \frac{\sigma_0^2 + \sigma^2}{\sigma^2 \sigma_0^2} (x - \mu)^2}
\end{align*}

This means that if we're given an initial estimate $x_0$ and a measurement $z_1$
with associated means and variances represented by Gaussian distributions, this
information can be combined into a third Gaussian distribution with its own mean
value and variance. The expected value of $x$ given $z_1$ is
\begin{equation}
  E[x|z_1] = \mu = \frac{\sigma_0^2}{\sigma_0^2 + \sigma^2}z_1 +
    \frac{\sigma^2}{\sigma_0^2 + \sigma^2}x_0
\end{equation}

The variance of $x$ given $z_1$ is
\begin{equation}
  E[(x - \mu)^2|z_1] = \frac{\sigma^2 \sigma_0^2}{\sigma_0^2 + \sigma^2}
\end{equation}

The expected value, which is also the maximum likelihood value, is the linear
combination of the prior expected (maximum likelihood) value and the
measurement. The expected value is a reasonable estimator of $x$.
\begin{align}
  \hat{x} &= E[x|z_1] = \frac{\sigma_0^2}{\sigma_0^2 + \sigma^2}z_1 +
    \frac{\sigma^2}{\sigma_0^2 + \sigma^2}x_0 \\
  \hat{x} &= w_1 z_1 + w_2 x_0 \nonumber
\end{align}

Note that the weights $w_1$ and $w_2$ sum to $1$. When the prior (i.e., prior
knowledge of \gls{state}) is uninformative (a large variance),
\begin{align}
  w_1 &= \lim_{\sigma_0^2 \to \infty} \frac{\sigma_0^2}{\sigma_0^2 + \sigma^2} = 1 \\
  w_2 &= \lim_{\sigma_0^2 \to \infty} \frac{\sigma^2}{\sigma_0^2 + \sigma^2} = 0
\end{align}

and $\hat{x} = z_1$. That is, the weight is on the observations and the estimate
is equal to the measurement.

Let us assume we have a \gls{model} providing an almost exact prior for $x$. In
that case, $\sigma_0^2$ approaches 0 and
\begin{align}
  w_1 &= \lim_{\sigma_0^2 \to 0} \frac{\sigma_0^2}{\sigma_0^2 + \sigma^2} = 0 \\
  w_2 &= \lim_{\sigma_0^2 \to 0} \frac{\sigma^2}{\sigma_0^2 + \sigma^2} = 1
\end{align}

The Kalman filter uses this optimal fusion as the basis for its operation.
