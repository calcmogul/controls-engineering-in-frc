\section{Linear stochastic systems}
\index{stochastic!linear systems}
\index{state-space observers!Kalman filter!derivations}

Given the following stochastic system
\begin{align*}
  \mat{x}_{k+1} &= \mat{A}\mat{x}_k + \mat{B}\mat{u}_k + \mat{w}_k \\
  \mat{y}_k &= \mat{C}\mat{x}_k + \mat{D}\mat{u}_k + \mat{v}_k
\end{align*}

where $\mat{w}_k$ is the process noise and $\mat{v}_k$ is the measurement noise,
\index{stochastic!process noise} \index{stochastic!measurement noise}
\begin{align*}
  E[\mat{w}_k] &= 0 \\
  E[\mat{w}_k\mat{w}_k^T] &= \mat{Q}_k \\
  E[\mat{v}_k] &= 0 \\
  E[\mat{v}_k\mat{v}_k^T] &= \mat{R}_k
\end{align*}

where $\mat{Q}_k$ is the process noise covariance matrix and $\mat{R}_k$ is the
measurement noise covariance matrix. We assume the noise samples are
independent, so $E[\mat{w}_k\mat{w}_j^T] = 0$ and $E[\mat{v}_k\mat{v}_k^T] = 0$
where $k \neq j$. Furthermore, process noise samples are independent from
measurement noise samples.

We'll compute the expectation of these equations and their covariance matrices,
which we'll use later for deriving the Kalman filter.

\subsection{State vector expectation evolution}

First, we will compute how the expectation of the \gls{system} \gls{state}
evolves.
\begin{align*}
  E[\mat{x}_{k+1}] &= E[\mat{A}\mat{x}_k + \mat{B}\mat{u}_k + \mat{w}_k] \\
  E[\mat{x}_{k+1}] &= E[\mat{A}\mat{x}_k] + E[\mat{B}\mat{u}_k] +
    E[\mat{w}_k] \\
  E[\mat{x}_{k+1}] &= \mat{A}E[\mat{x}_k] + \mat{B}E[\mat{u}_k] +
    E[\mat{w}_k] \\
  E[\mat{x}_{k+1}] &= \mat{A}E[\mat{x}_k] + \mat{B}\mat{u}_k + 0 \\
  \matbar{x}_{k+1} &= \mat{A}\matbar{x}_k + \mat{B}\mat{u}_k \\
\end{align*}

\subsection{Error covariance matrix evolution}

Now, we will use this to compute how the \gls{error} covariance matrix $\mat{P}$
evolves.
\begin{align*}
  \mat{x}_{k+1} - \matbar{x}_{k+1} &= \mat{A}\mat{x}_k +
    \mat{B}\mat{u}_k + \mat{w}_k - (\mat{A}\matbar{x}_k - \mat{B}\mat{u}_k) \\
  \mat{x}_{k+1} - \matbar{x}_{k+1} &=
    \mat{A}(\mat{x}_k - \matbar{x}_k) + \mat{w}_k
\end{align*}
\begin{equation*}
  E[(\mat{x}_{k+1} - \matbar{x}_{k+1})(\mat{x}_{k+1} - \matbar{x}_{k+1})^T] =
    E[(\mat{A}(\mat{x}_k - \matbar{x}_k) + \mat{w}_k)
      (\mat{A}(\mat{x}_k - \matbar{x}_k) + \mat{w}_k)^T]
\end{equation*}
\begin{align*}
  \mat{P}_{k+1} &=
    E[(\mat{A}(\mat{x}_k - \matbar{x}_k) + \mat{w}_k)
      (\mat{A}(\mat{x}_k - \matbar{x}_k) + \mat{w}_k)^T] \\
  \mat{P}_{k+1} &=
    E[(\mat{A}(\mat{x}_k - \matbar{x}_k)(\mat{x}_k - \matbar{x}_k)^T
      \mat{A}^T] +
    E[\mat{A}(\mat{x}_k - \matbar{x}_k)\mat{w}_k^T] \\
    &\qquad + E[\mat{w}_k(\mat{x}_k - \matbar{x}_k)^T\mat{A}^T] +
    E[\mat{w}_k\mat{w}_k^T] \\
  \mat{P}_{k+1} &=
    \mat{A}E[(\mat{x}_k - \matbar{x}_k)(\mat{x}_k - \matbar{x}_k)^T]
    \mat{A}^T +
    \mat{A}E[(\mat{x}_k - \matbar{x}_k)\mat{w}_k^T] \\
    &\qquad + E[\mat{w}_k(\mat{x}_k - \matbar{x}_k)^T]\mat{A}^T +
    E[\mat{w}_k\mat{w}_k^T] \\
  \mat{P}_{k+1} &= \mat{A}\mat{P}_k\mat{A}^T +
    \mat{A}E[(\mat{x}_k - \matbar{x}_k)\mat{w}_k^T] +
    E[\mat{w}_k(\mat{x}_k - \matbar{x}_k)^T]\mat{A}^T + \mat{Q}_k
\end{align*}

Since the error and noise are independent, the cross terms are zero.
\begin{align*}
  \mat{P}_{k+1} &= \mat{A}\mat{P}_k\mat{A}^T +
    \mat{A}\underbrace{
      E[(\mat{x}_k - \matbar{x}_k)\mat{w}_k^T]}_\mat{0} +
    \underbrace{
      E[\mat{w}_k(\mat{x}_k - \matbar{x}_k)^T]}_\mat{0}\mat{A}^T + \mat{Q}_k \\
  \mat{P}_{k+1} &= \mat{A}\mat{P}_k\mat{A}^T + \mat{Q}_k
\end{align*}

\subsection{Measurement vector expectation}

Next, we will compute the expectation of the \gls{output} $\mat{y}$.
\begin{align*}
  E[\mat{y}_k] &= E[\mat{C}\mat{x}_k + \mat{D}\mat{u}_k + \mat{v}_k] \\
  E[\mat{y}_k] &= \mat{C}E[\mat{x}_k] + \mat{D}\mat{u}_k + 0 \\
  \matbar{y}_k &= \mat{C}\matbar{x}_k + \mat{D}\mat{u}_k
\end{align*}

\subsection{Measurement covariance matrix}

Now, we will use this to compute how the measurement covariance matrix
$\mat{S}$ evolves.
\begin{align*}
  \mat{y}_k - \matbar{y}_k &= \mat{C}\mat{x}_k + \mat{D}\mat{u}_k + \mat{v}_k -
    (\mat{C}\matbar{x}_k + \mat{D}\mat{u}_k) \\
  \mat{y}_k - \matbar{y}_k &= \mat{C}(\mat{x}_k - \matbar{x}_k) + \mat{v}_k \\
  E[(\mat{y}_k - \matbar{y}_k)(\mat{y}_k - \matbar{y}_k)^T] &=
    E[(\mat{C}(\mat{x}_k - \matbar{x}_k) + \mat{v}_k)
      (\mat{C}(\mat{x}_k - \matbar{x}_k) + \mat{v}_k)^T]
\end{align*}
\begin{align*}
  \mat{S}_k &=
    E[(\mat{C}(\mat{x}_k - \matbar{x}_k) + \mat{v}_k)
      (\mat{C}(\mat{x}_k - \matbar{x}_k) + \mat{v}_k)^T] \\
  \mat{S}_k &=
    E[(\mat{C}(\mat{x}_k - \matbar{x}_k) + \mat{v}_k)
      ((\mat{x}_k - \matbar{x}_k)^T\mat{C}^T + \mat{v}_k^T)] \\
  \mat{S}_k &=
    E[(\mat{C}(\mat{x}_k - \matbar{x}_k)(\mat{x}_k - \matbar{x}_k)^T
      \mat{C}^T] +
    E[\mat{C}(\mat{x}_k - \matbar{x}_k)\mat{v}_k^T] \\
    &\qquad + E[\mat{v}_k(\mat{x}_k - \matbar{x}_k)^T\mat{C}^T] +
    E[\mat{v}_k\mat{v}_k^T] \\
  \mat{S}_k &=
    \mat{C}E[(\mat{x}_k - \matbar{x}_k)(\mat{x}_k - \matbar{x}_k)^T]
    \mat{C}^T + \mat{C}E[(\mat{x}_k - \matbar{x}_k)\mat{v}_k^T] \\
    &\qquad + E[\mat{v}_k(\mat{x}_k - \matbar{x}_k)^T]\mat{C}^T +
    E[\mat{v}_k\mat{v}_k^T] \\
  \mat{S}_k &= \mat{C}\mat{P}_k\mat{C}^T +
    \mat{C}E[(\mat{x}_k - \matbar{x}_k)\mat{v}_k^T] +
    E[\mat{v}_k(\mat{x}_k - \matbar{x}_k)^T]\mat{C}^T +
    \mat{R}_k
\end{align*}

Since the error and noise are independent, the cross terms are zero.
\begin{align*}
  \mat{S}_k &= \mat{C}\mat{P}_k\mat{C}^T +
    \mat{C}\underbrace{E[(\mat{x}_k - \matbar{x}_k)\mat{v}_k^T]}_\mat{0} +
    \underbrace{
      E[\mat{v}_k(\mat{x}_k - \matbar{x}_k)^T]}_\mat{0}\mat{C}^T + \mat{R}_k \\
  \mat{S}_k &= \mat{C}\mat{P}_k\mat{C}^T + \mat{R}_k
\end{align*}
