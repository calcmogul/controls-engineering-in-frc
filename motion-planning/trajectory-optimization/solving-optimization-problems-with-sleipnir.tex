\section{Solving optimization problems with Sleipnir}

Sleipnir is a C++ library that facilitates the specification of constrained
nonlinear optimization problems with natural mathematical notation, then
efficiently solves them.

Sleipnir supports problems of the form:
\begin{align*}
  \min_x             &f(x) \\
  \text{subject to } &c_e(x) = 0 \\
                     &c_i(x) \geq 0
\end{align*}

where $f(x)$ is the scalar cost function, $x$ is the vector of decision
variables (variables the solver can tweak to minimize the cost function),
$c_e(x)$ is the vector-valued function whose rows are equality constraints, and
$c_i(x)$ is the vector-valued function whose rows are inequality constraints.
Constraints are equations or inequalities of the decision variables that
constrain what values the solver is allowed to use when searching for an optimal
solution.

The nice thing about Sleipnir is users don't have to put their system in the
form shown above manually; they can write it in natural mathematical form and
it'll be converted for them. We'll cover some examples next.

\subsection{Double integrator}

A system with position and velocity states and an acceleration input is an
example of a double integrator. We want to go from $0$ m at rest to $10$ m at
rest in the minimum time while obeying the velocity limit $(-1, 1)$ and the
acceleration limit $(-1, 1)$.

The model for our double integrator is $\ddot{x} = u$ where $x$ is the vector
$\begin{bmatrix}\text{position} & \text{velocity}\end{bmatrix}\T$ and $u$ is the
acceleration. The velocity constraints are $-1 \leq x_1 \leq 1$ and the
acceleration constraints are $-1 \leq u \leq 1$.

\subsubsection{Importing required libraries}

Sleipnir can be installed via \texttt{pip install sleipnirgroup-jormungandr}.
\begin{code}
  \begin{lstlisting}[style=customPython]
    from sleipnir.optimization import Problem, bounds
    import numpy as np
  \end{lstlisting}
\end{code}

\subsubsection{Initializing a problem instance}

First, we need to make a problem instance.
\begin{coderemotesubset}{Python}{\chapterpath/double_integrator.py}{22}{28}
\end{coderemotesubset}

\subsubsection{Creating decision variables}

Next, we need to make decision variables for our state and input.
\begin{coderemotesubset}{Python}{\chapterpath/double_integrator.py}{30}{34}
\end{coderemotesubset}

By convention, we use capital letters for the variables to designate
matrices.

\subsubsection{Applying constraints}

Now, we need to apply dynamics constraints between timesteps.
\begin{coderemotesubset}{Python}{\chapterpath/double_integrator.py}{36}{45}
\end{coderemotesubset}

Next, we'll apply the state and input constraints.
\begin{coderemotesubset}{Python}{\chapterpath/double_integrator.py}{47}{57}
\end{coderemotesubset}

\subsubsection{Specifying a cost function}

Next, we'll create a cost function for minimizing position error.
\begin{coderemotesubset}{Python}{\chapterpath/double_integrator.py}{59}{60}
\end{coderemotesubset}

The cost function passed to minimize() should produce a scalar output.

\subsubsection{Solving the problem}

Now we can solve the problem.
\begin{coderemotesubset}{Python}{\chapterpath/double_integrator.py}{62}{62}
\end{coderemotesubset}

The solver will find the decision variable values that minimize the cost
function while satisfying the constraints.

\subsubsection{Accessing the solution}

You can obtain the solution by querying the values of the variables like so.
\begin{code}
  \begin{lstlisting}[style=customPython]
    position = X.value[0, 0]
    velocity = X.value[1, 0]
    acceleration = U.value(0)
  \end{lstlisting}
\end{code}
\begin{bookfigure}
  \begin{minisvg}{2}{build/\chapterpath/double_integrator_position}
    \caption{Double integrator position}
  \end{minisvg}
  \hfill
  \begin{minisvg}{2}{build/\chapterpath/double_integrator_velocity}
    \caption{Double integrator velocity}
  \end{minisvg}
  \hfill
  \begin{minisvg}{2}{build/\chapterpath/double_integrator_acceleration}
    \caption{Double integrator acceleration}
  \end{minisvg}
\end{bookfigure}

\subsubsection{Other applications}

In retrospect, the solution here seems obvious: if you want to reach the desired
position in the minimum time, you just apply positive max input to accelerate to
the max speed, coast for a while, then apply negative max input to decelerate to
a stop at the desired position. Optimization problems can get more complex than
this though. In fact, we can use this same framework to design optimal
trajectories for a drivetrain while satisfying dynamics constraints, avoiding
keep-out regions, and driving through points of interest.

Matthew Kelly has an approachable introduction to trajectory optimization on his
website \cite{bib:intro_to_traj_opt}.

\subsection{Optimizing the problem formulation}

Cost functions and constraints can have the following orders:
\begin{itemize}
  \item none (i.e., there is no cost function or are no constraints)
  \item constant
  \item linear
  \item quadratic
  \item nonlinear
\end{itemize}

For nonlinear problems, the solver calculates the Hessian of the cost function
and the Jacobians of the constraints at each iteration. However, problems with
lower order cost functions and constraints can be solved faster. For example,
the following only need to be computed once because they're constant:
\begin{itemize}
  \item the Hessian of a quadratic or lower cost function
  \item the Jacobian of linear or lower constraints
\end{itemize}

A problem is constant if:
\begin{itemize}
  \item the cost function is constant or lower
  \item the equality constraints are constant or lower
  \item the inequality constraints are constant or lower
\end{itemize}

A problem is a linear program (LP) if:
\begin{itemize}
  \item the cost function is linear
  \item the equality constraints are linear or lower
  \item the inequality constraints are linear or lower
\end{itemize}

A problem is a quadratic program (QP) if:
\begin{itemize}
  \item the cost function is quadratic
  \item the equality constraints are linear or lower
  \item the inequality constraints are linear or lower
\end{itemize}

All other problems are nonlinear programs (NLPs).
