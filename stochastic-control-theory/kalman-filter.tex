\section{Kalman filter}

So far, we've derived equations for updating the expected value and state
covariance without measurements and how to incorporate measurements into an
initial state optimally. Now, we'll combine these concepts to produce an
estimator which minimizes the error covariance for linear systems.

\subsection{Derivations}
\index{State-space observers!Kalman filter!derivations}

Given the posteriori update equation $\hat{\mtx{x}}_{k+1}^+ =
\hat{\mtx{x}}_{k+1}^- + \mtx{K}_{k+1}(\mtx{y}_{k+1} -
\mtx{H}_{k+1} \hat{\mtx{x}}_{k+1}^-)$, we want to find the value of $\mtx{K}$
that minimizes the error covariance, because doing this minimizes the estimation
error.

\subsubsection{\textit{a posteriori} estimate covariance matrix}

\begin{align*}
  \mtx{P}_{k+1}^+ &= cov(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^+) \\
  \mtx{P}_{k+1}^+ &= cov(\mtx{x}_{k+1} - (\hat{\mtx{x}}_{k+1}^- +
    \mtx{K}_{k+1}(\mtx{y}_{k+1} - \mtx{H} \hat{\mtx{x}}_{k+1}^-))) \\
  \mtx{P}_{k+1}^+ &= cov(\mtx{x}_{k+1} - (\hat{\mtx{x}}_{k+1}^- +
    \mtx{K}_{k+1}(\mtx{H}_{k+1}\mtx{x}_{k+1} + \mtx{v}_{k+1} -
    \mtx{H} \hat{\mtx{x}}_{k+1}^-))) \\
  \mtx{P}_{k+1}^+ &= cov(\mtx{x}_{k+1} - (\hat{\mtx{x}}_{k+1}^- +
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{x}_{k+1} + \mtx{K}_{k+1}\mtx{v}_{k+1} -
    \mtx{K}_{k+1}\mtx{H} \hat{\mtx{x}}_{k+1}^-)) \\
  \mtx{P}_{k+1}^+ &= cov(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^- -
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{x}_{k+1} - \mtx{K}_{k+1}\mtx{v}_{k+1} +
    \mtx{K}_{k+1}\mtx{H} \hat{\mtx{x}}_{k+1}^-) \\
  \mtx{P}_{k+1}^+ &= cov(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^- -
    \mtx{K}_{k+1}\mtx{H}_{k+1}(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-) -
    \mtx{K}_{k+1}\mtx{v}_{k+1}) \\
  \mtx{P}_{k+1}^+ &= cov((\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})
    (\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-) - \mtx{K}_{k+1}\mtx{v}_{k+1}) \\
  \mtx{P}_{k+1}^+ &= cov((\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})
    (\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-)) + cov(\mtx{K}_{k+1}\mtx{v}_{k+1})
    \\
  \mtx{P}_{k+1}^+ &= (\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})
    cov(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-)
    (\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})^T + \mtx{K}_{k+1}cov(\mtx{v}_{k+1})
    \mtx{K}_{k+1}^T \\
  \mtx{P}_{k+1}^+ &= (\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})\mtx{P}_{k+1}^-
    (\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})^T + \mtx{K}_{k+1}\mtx{R}_{k+1}
    \mtx{K}_{k+1}^T \\
\end{align*}

\subsubsection{Kalman gain}

The error in the \textit{a posteriori} state estimation is
$\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-$. We want to minimize the expected value
of the square of the magnitude of this vector. This is equivalent to minimizing
the trace of the a posteriori estimate covariance matrix $\mtx{P}_{k+1}^-$.

First, we'll expand the equation for $\mtx{P}_{k+1}^-$ and collect terms.

\begin{align}
  \mtx{P}_{k+1}^+ =~& (\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})\mtx{P}_{k+1}^-
    (\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})^T + \mtx{K}_{k+1}\mtx{R}_{k+1}
    \mtx{K}_{k+1}^T \nonumber \\
  \mtx{P}_{k+1}^+ =~&
    (\mtx{P}_{k+1}^- - \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-)
    (\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})^T + \mtx{K}_{k+1}\mtx{R}_{k+1}
    \mtx{K}_{k+1}^T \nonumber \\
  \mtx{P}_{k+1}^+ =~&
    (\mtx{P}_{k+1}^- - \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-)
    (\mtx{I}^T - \mtx{H}_{k+1}^T\mtx{K}_{k+1}^T) +
    \mtx{K}_{k+1}\mtx{R}_{k+1} \mtx{K}_{k+1}^T \nonumber \\
  \mtx{P}_{k+1}^+ =~&
    \mtx{P}_{k+1}^-(\mtx{I}^T - \mtx{H}_{k+1}^T\mtx{K}_{k+1}^T) -
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-
    (\mtx{I}^T - \mtx{H}_{k+1}^T\mtx{K}_{k+1}^T) +
    \mtx{K}_{k+1}\mtx{R}_{k+1} \mtx{K}_{k+1}^T \nonumber \\
  \mtx{P}_{k+1}^+ =~&
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^- +
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{K}_{k+1}^T +
      \nonumber \\
    &\mtx{K}_{k+1}\mtx{R}_{k+1} \mtx{K}_{k+1}^T \nonumber \\
  \mtx{P}_{k+1}^+ =~&
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^- + \nonumber \\
    &\mtx{K}_{k+1}(\mtx{H}_{k+1}\mtx{P}_{k+1}^-\mtx{H}_{k+1}^T +
    \mtx{R}_{k+1})\mtx{K}_{k+1}^T \nonumber \\
  \mtx{P}_{k+1}^+ =~&
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^- +
    \mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T \label{eq:post_p_update}
\end{align}

Now we'll take the trace.

\begin{equation*}
  \tr(\mtx{P}_{k+1}^+) =
    \tr(\mtx{P}_{k+1}^-) - \tr(\mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{K}_{k+1}^T) -
    \tr(\mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T)
\end{equation*}

Transpose one of the terms twice.

\begin{equation*}
  \tr(\mtx{P}_{k+1}^+) = \tr(\mtx{P}_{k+1}^-) -
    \tr((\mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^{-T})^T) -
    \tr(\mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T)
\end{equation*}

$\mtx{P}_{k+1}^-$ is symmetric, so we can drop the transpose.

\begin{equation*}
  \tr(\mtx{P}_{k+1}^+) = \tr(\mtx{P}_{k+1}^-) -
    \tr((\mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-)^T) -
    \tr(\mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T)
\end{equation*}

The trace of a matrix is equal to the trace of its transpose since the elements
used in the trace are on the diagonal.

\begin{align*}
  \tr(\mtx{P}_{k+1}^+) &= \tr(\mtx{P}_{k+1}^-) -
    \tr(\mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-) -
    \tr(\mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T) \\
  \tr(\mtx{P}_{k+1}^+) &= \tr(\mtx{P}_{k+1}^-) -
    2\tr(\mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T)
\end{align*}

Given theorems \ref{thm:partial_tr_aba} and \ref{thm:partial_tr_ac}

\begin{theorem}
  $\frac{\partial}{\partial\mtx{A}}\tr(\mtx{A}\mtx{B}\mtx{A}^T) =
    2\mtx{A}\mtx{B}$ where $\mtx{B}$ is symmetric.
  \label{thm:partial_tr_aba}
\end{theorem}

\begin{theorem}
  $\frac{\partial}{\partial\mtx{A}}\tr(\mtx{A}\mtx{C}) = \mtx{C}^T$
  \label{thm:partial_tr_ac}
\end{theorem}

find the minimum of the trace of $\mtx{P}_{k+1}^+$ by taking the partial
derivative with respect to $\mtx{K}$ and setting the result to $\mtx{0}$.

\begin{align*}
  \frac{\partial\tr(\mtx{P}_{k+1}^+)}{\partial\mtx{K}} &=
    \mtx{0} - 2(\mtx{H}_{k+1}\mtx{P}_{k+1}^-)^T + 2\mtx{K}_{k+1}\mtx{S}_{k+1} \\
  \frac{\partial\tr(\mtx{P}_{k+1}^+)}{\partial\mtx{K}} &=
    -2\mtx{P}_{k+1}^{-T}\mtx{H}_{k+1}^T + 2\mtx{K}_{k+1}\mtx{S}_{k+1} \\
  \frac{\partial\tr(\mtx{P}_{k+1}^+)}{\partial\mtx{K}} &=
    -2\mtx{P}_{k+1}^-\mtx{H}_{k+1}^T + 2\mtx{K}_{k+1}\mtx{S}_{k+1} \\
  \mtx{0} &= -2\mtx{P}_{k+1}^-\mtx{H}_{k+1}^T + 2\mtx{K}_{k+1}\mtx{S}_{k+1} \\
  2\mtx{K}_{k+1}\mtx{S}_{k+1} &= 2\mtx{P}_{k+1}^-\mtx{H}_{k+1}^T \\
  \mtx{K}_{k+1}\mtx{S}_{k+1} &= \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T \\
  \mtx{K}_{k+1} &= \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{S}_{k+1}^{-1}
\end{align*}

This is the optimal Kalman gain.

\subsubsection{Simplified \textit{a priori} estimate covariance matrix}

If the optimal Kalman gain is used, the a posteriori estimate covariance matrix
update equation can be simplified. First, we'll manipulate the equation for the
optimal Kalman gain.

\begin{align*}
  \mtx{K}_{k+1} &= \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{S}_{k+1}^{-1} \\
  \mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T &=
    \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{K}_{k+1}^T
\end{align*}

Now we'll substitute it into equation (\ref{eq:post_p_update}).

\begin{align*}
  \mtx{P}_{k+1}^+ =~&
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^- +
    \mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T \\
  \mtx{P}_{k+1}^+ &=
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^- +
    \mtx{P}_{k+1}^-\mtx{H}_{k+1}^T\mtx{K}_{k+1}^T \\
  \mtx{P}_{k+1}^+ &= \mtx{P}_{k+1}^- -
    \mtx{K}_{k+1}\mtx{H}_{k+1}\mtx{P}_{k+1}^- \\
  \mtx{P}_{k+1}^+ &= (\mtx{I} - \mtx{K}_{k+1}\mtx{H}_{k+1})\mtx{P}_{k+1}^-
\end{align*}

\subsection{Predict and update equations}

Now that we've derived all the pieces we need, we can finally write all the
equations for a Kalman filter. Theorem \ref{thm:kalman_filter} shows the predict
and update steps for a Kalman filter at the $k^{th}$ timestep.

\index{State-space observers!Kalman filter!equations}
\begin{theorem}[Kalman filter]
  \begin{align}
    \text{Predict step} \nonumber \\
    \hat{\mtx{x}}_{k+1}^- &= \mtx{\Phi}\hat{\mtx{x}}_k + \mtx{B} \mtx{u}_k
      \label{eq:pre1_x} \\
    \mtx{P}_{k+1}^- &= \mtx{\Phi} \mtx{P}_k^- \mtx{\Phi}^T +
      \mtx{\Gamma}\mtx{Q}\mtx{\Gamma}^T \\
    \text{Update step} \nonumber \\
    \mtx{K}_{k+1} &=
      \mtx{P}_{k+1}^- \mtx{H}^T (\mtx{H}\mtx{P}_{k+1}^- \mtx{H}^T +
      \mtx{R})^{-1} \\
    \hat{\mtx{x}}_{k+1}^+ &=
      \hat{\mtx{x}}_{k+1}^- + \mtx{K}_{k+1}(\mtx{y}_{k+1} -
      \mtx{H} \hat{\mtx{x}}_{k+1}^-) \label{eq:post1_x} \\
    \mtx{P}_{k+1}^+ &= (\mtx{I} - \mtx{K}_{k+1}\mtx{H})\mtx{P}_{k+1}^-
  \end{align}

  \begin{figurekey}
    \begin{tabulary}{\linewidth}{LLLL}
      $\mtx{\Phi}$ & system matrix & $\hat{\mtx{x}}$ & state estimate vector \\
      $\mtx{B}$ & input matrix            & $\mtx{u}$ & input vector \\
      $\mtx{H}$ & measurement matrix      & $\mtx{y}$ & output vector \\
      $\mtx{P}$ & error covariance matrix & $\mtx{Q}$ & process noise covariance
        matrix \\
      $\mtx{K}$ & Kalman gain matrix & $\mtx{R}$ & measurement noise covariance
        matrix \\
      $\mtx{\Gamma}$ & process noise intensity vector &
    \end{tabulary}
  \end{figurekey}

  where a superscript of minus denotes \textit{a priori} and plus denotes
  \textit{a posteriori} estimate (before and after update respectively).

  \label{thm:kalman_filter}
\end{theorem}

$\mtx{H}$, $\mtx{Q}$, and $\mtx{R}$ from the equations derived earlier are made
constants here. $\mtx{\Phi}$ is replaced with $\mtx{A}$ for continuous systems.

\begin{booktable}
  \begin{tabular}{|ll|ll|}
    \hline
    \rowcolor{headingbg}
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} &
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} \\
    \hline
    $\mtx{\Phi}$ & states $\times$ states & $\hat{\mtx{x}}$ & states $\times$ 1
      \\
    $\mtx{B}$ & states $\times$ inputs & $\mtx{u}$ & inputs $\times$ 1 \\
    $\mtx{H}$ & outputs $\times$ states & $\mtx{y}$ & outputs $\times$ 1 \\
    $\mtx{P}$ & states $\times$ states & $\mtx{Q}$ & states $\times$ states \\
    $\mtx{K}$ & states $\times$ outputs & $\mtx{R}$ & outputs $\times$ outputs
      \\
    $\mtx{\Gamma}$ & states $\times$ 1 &  &  \\
    \hline
  \end{tabular}
  \caption{Kalman filter matrix dimensions}
  \label{tab:kf_matrix_dims}
\end{booktable}

Unknown states in a Kalman filter are generally represented by a Wiener
(pronounced VEE-ner) process\footnote{Explaining why we use the Wiener process
would require going much more in depth into stochastic processes and It\^{o}
calculus, which is outside the scope of this book.}. This process has the
property that its variance increases linearly with time $t$.

\subsection{Equations to model}
\index{State-space observers!Kalman filter!setup}

The following example system will be used to describe how to define and
initialize the matrices for a Kalman filter.

A robot is between two parallel walls. It starts driving from one wall to the
other at a velocity of $0.8 cm/s$ and uses ultrasonic sensors to provide noisy
measurements of the distances to the walls in front of and behind it. To
estimate the distance between the walls, we will define three states: robot
position, robot velocity, and distance between the walls.

\begin{align}
  x_{k+1} &= x_k + v_k \Delta T \\
  v_{k+1} &= v_k \\
  x_{k+1}^w &= x_k^w
\end{align}

This can be converted to the following state-space \gls{model}.

\begin{equation}
  \mtx{x}_k =
  \begin{bmatrix}
    x_k \\
    v_k \\
    x_k^w
  \end{bmatrix}
\end{equation}

\begin{equation}
  \mtx{x}_{k+1} =
  \begin{bmatrix}
    1 & 1 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 1
  \end{bmatrix} \mtx{x}_k +
  \begin{bmatrix}
    0 \\
    0.8 \\
    0
  \end{bmatrix} +
  \begin{bmatrix}
    0 \\
    0.1 \\
    0
  \end{bmatrix} w_k
\end{equation}

where the Gaussian random variable $w_k$ has zero mean and variance 1. The
observation \gls{model} is

\begin{equation}
  \mtx{y}_k =
  \begin{bmatrix}
    1 & 0 & 0 \\
    -1 & 0 & 1
  \end{bmatrix} \mtx{x}_k + \theta_k
\end{equation}

where the covariance matrix of Gaussian measurement noise $\theta$ is a
$2 \times 2$ matrix with both diagonals $10 cm^2$.

The state vector is usually initialized using the first measurement or two. The
covariance matrix entries are assigned by calculating the covariance of the
expressions used when assigning the state vector. Let $k = 2$.

\begin{align}
  \mtx{Q} &= \begin{bmatrix}1\end{bmatrix} \\
  \mtx{R} &=
  \begin{bmatrix}
    10 & 0 \\
    0 & 10
  \end{bmatrix} \\
  \hat{\mtx{x}} &=
  \begin{bmatrix}
    \mtx{y}_{k,1} \\
    (\mtx{y}_{k,1} - \mtx{y}_{k-1,1})/dt \\
    \mtx{y}_{k,1} + \mtx{y}_{k,2}
  \end{bmatrix} \\
  \mtx{P} &=
  \begin{bmatrix}
    10 & 10/dt & 10 \\
    10/dt & 20/dt^2 & 10/dt \\
    10 & 10/dt & 20
  \end{bmatrix}
\end{align}

\subsection{Initial conditions}

To fill in the $\mtx{P}$ matrix, we calculate the covariance of each combination
of state variables. The resulting value is a measure of how much those variables
are correlated. Due to how the covariance calculation works out, the covariance
between two variables is the sum of the variance of matching terms which aren't
constants multiplied by any constants the two have. If no terms match, the
variables are uncorrelated and the covariance is zero.

In $\mtx{P}_{11}$, the terms in $\mtx{x}_1$ correlate with itself. Therefore,
$\mtx{P}_{11}$ is $\mtx{x}_1$'s variance, or $\mtx{P}_{11} = 10$. For
$\mtx{P}_{21}$, One term correlates between $\mtx{x}_1$ and $\mtx{x}_2$, so
$\mtx{P}_{21} = \frac{10}{dt}$. The constants from each are simply multiplied
together. For $\mtx{P}_{22}$, both measurements are correlated, so the variances
add together. Therefore, $\mtx{P}_{22} = \frac{20}{dt^2}$. It continues in this
fashion until the matrix is filled up. Order doesn't matter for correlation, so
the matrix is symmetric.

\subsection{Selection of priors}

Choosing good priors is important for a well performing filter, even if little
information is known. This applies to both the measurement noise and the noise
\gls{model}. The act of giving a state variable a large variance means you know
something about the system. Namely, you aren't sure whether your initial guess
is close to the true state. If you make a guess and specify a small variance,
you are telling the filter that you are very confident in your guess. If that
guess is incorrect, it will take the filter a long time to move away from your
guess to the true value.

\subsection{Covariance selection}

While one could assume no correlation between the state variables and set the
covariance matrix entries to zero, this may not reflect reality. The Kalman
filter is still guarenteed to converge to the steady-state covariance after an
infinite time, but it will take longer than otherwise.

\subsection{Noise model selection}

We typically use a Gaussian distribution for the noise \gls{model} because the
sum of many independent random variables produces a normal distribution by the
central limit theorem. Kalman filters only require that the noise has a zero
mean. If the true value has an equal probability of being anywhere within a
certain range, use a uniform distribution instead. Each of these communicates
information regarding what you know about a system in addition to what you do
not.

\subsection{Simulation}

Figure \ref{fig:kalman_robot_all} shows the state estimates and measurements of
the Kalman filter over time. Figure \ref{fig:kalman_robot_position} shows the
position estimate and variance over time. Figure \ref{fig:kalman_robot_wall}
shows the wall position estimate and variance over time. Notice how the
variances decrease over time as the filter gathers more measurements. This means
that the filter becomes more confident in its state estimates.

The final precisions in estimating the position of the robot and the wall are
the square roots of the corresponding elements in the covariance matrix. That
is, $0.5188\,m$ and $0.4491\,m$ respectively. They are smaller than the
precision of the raw measurements, $\sqrt{10} = 3.1623\,m$. As expected,
combining the information from several measurements produces a better estimate
than any one measurement alone.

\begin{svg}{build/code/kalman_robot_all}
  \caption{State estimates and measurements}
  \label{fig:kalman_robot_all}
\end{svg}
\begin{svg}{build/code/kalman_robot_position}
  \caption{Robot position estimate and variance}
  \label{fig:kalman_robot_position}
\end{svg}
\begin{svg}{build/code/kalman_robot_wall}
  \caption{Wall position estimate and variance}
  \label{fig:kalman_robot_wall}
\end{svg}

\subsection{Steady-state error covariance matrix}

One may have noticed that the error covariance matrix can be updated
independently of the rest of the model. The error covariance matrix tends
toward a steady-state value, and this matrix can be obtained via the discrete
algebraic Ricatti equation. This can then be used to compute a steady-state
Kalman gain.

Snippet \ref{lst:kalman} computes the steady-state matrices for a Kalman
filter.

\begin{code}{Python}{build/frccontrol/frccontrol/kalmd.py}
  \caption{Steady-state Kalman gain and error covariance matrices calculation in
    Python}
  \label{lst:kalman}
\end{code}

\subsection{Kalman filter as Luenberger observer}
\index{State-space observers!Kalman filter!as Luenberger observer}

A Kalman filter can be represented as a Luenberger observer by letting
$\mtx{C} = \mtx{H}$ and $\mtx{L} = \mtx{A} \mtx{K}_k$ (see appendix
\ref{sec:deriv-kalman-luenberger}). The eigenvalues of the Kalman filter are

\begin{equation}
  eig(\mtx{A}(\mtx{I} - \mtx{K}_k\mtx{H}))
\end{equation}
